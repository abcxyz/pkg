# Copyright 2023 The Authors (see AUTHORS file)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: 'Deploy Dataflow Pipeline'
description: 'Deploys a Dataflow pipeline to GCP'

inputs:
  wif_provider:
    description: 'The WIF provider to use when authenticating with GCP'
    required: true
  wif_service_account:
    description: 'The WIF service account to use when authenticating with GCP'
    required: true
  image:
    description: 'The container image that contains the pipeline to execute.'
    required: true
  template_metadata_file:
    description: 'The local file containing the template metadata defining the pipeline.'
    required: true
  template_file_name:
    description: 'The name of the compiled template file that will be stored in GCS.'
    required: true
  project:
    description: 'The GCP Project to use.'
    required: true
  region:
    description: 'The GCP region to use.'
    required: true
  bucket_name:
    description: 'The gcs bucket to store the template file.'
    required: true
  network_name:
    description: 'The name of GCP network to use.'
    required: true
  subnet_name:
    description: 'Then name of the subnet to use.'
    required: false
  pipeline_name:
    description: 'The name of the pipeline.'
    required: true
  pipeline_sdk:
    description: 'SDK language of the pipeline. Must be one of: JAVA, PYTHON, GO.'
    required: true
  pipeline_type:
    description: 'The type of pipeline. Must be one of: BATCH, STREAMING.'
    required: true
  pipeline_schedule:
    description: 'The cron formatted schedule for the pipeline.'
    required: true
  pipeline_experiments:
    description: 'Experiment flags that should be enabled for the pipeline. Default: use_prime'
    default: 'use_prime'
    required: false
  pipeline_parameters:
    description: 'The command line parameters for the pipeline.'
    required: false


runs:
  using: 'composite'
  steps:
    - name: 'Checkout'
      uses: 'actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683' # ratchet:actions/checkout@v4

    - id: 'auth'
      name: 'Authenticate to Google Cloud'
      uses: 'google-github-actions/auth@6fc4af4b145ae7821d527454aa9bd537d1f2dc5f' # ratchet:google-github-actions/auth@v2
      with:
        workload_identity_provider: '${{ inputs.wif_provider }}'
        service_account: '${{ inputs.wif_service_account }}'

    - name: 'Setup gcloud'
      uses: 'google-github-actions/setup-gcloud@6189d56e4096ee891640bb02ac264be376592d6a' # ratchet:google-github-actions/setup-gcloud@v2
      with:
        install_components: 'beta'

    - name: 'Deploy Dataflow Pipeline'
      shell: 'bash'
      env:
        IMAGE: '${{ inputs.image }}'
        TEMPLATE_METADATA_FILE: '${{ inputs.template_metadata_file }}'
        TEMPLATE_FILE_NAME: '${{ inputs.template_file_name }}'
        PROJECT: '${{ inputs.project }}'
        REGION: '${{ inputs.region }}'
        NETWORK_NAME: '${{ inputs.network_name }}'
        SUBNET_NAME: '${{ inputs.subnet_name }}'
        BUCKET_NAME: '${{ inputs.bucket_name }}'
        PIPELINE_NAME: '${{ inputs.pipeline_name }}'
        PIPELINE_SDK: '${{ inputs.pipeline_sdk }}'
        PIPELINE_TYPE: '${{ inputs.pipeline_type }}'
        PIPELINE_SCHEDULE: '${{ inputs.pipeline_schedule }}'
        PIPELINE_EXPERIMENTS: '${{ inputs.pipeline_experiments }}'
        PIPELINE_PARAMETERS: '${{ inputs.pipeline_parameters }}'
      run: |-
        GCS_TEMPLATE_FILE="gs://${BUCKET_NAME}/templates/${TEMPLATE_FILE_NAME}"

        gcloud dataflow flex-template build "${GCS_TEMPLATE_FILE}" \
          --project="${PROJECT}" \
          --image="${IMAGE}" \
          --sdk-language="${PIPELINE_SDK}" \
          --metadata-file="${TEMPLATE_METADATA_FILE}"

        gcloud beta datapipelines pipeline delete "${PIPELINE_NAME}" \
          --project="${PROJECT}" \
          --region="${REGION}" || echo "no pipeline with ${PIPELINE_NAME} to delete"

        gcloud beta datapipelines pipeline create "${PIPELINE_NAME}" \
          --project="${PROJECT}" \
          --region="${REGION}" \
          --pipeline-type="${PIPELINE_TYPE}" \
          --template-file-gcs-location="${GCS_TEMPLATE_FILE}" \
          --network="${NETWORK_NAME}" \
          --subnetwork="https://www.googleapis.com/compute/v1/projects/${PROJECT}/regions/${REGION}/subnetworks/${SUBNET_NAME}" \
          --schedule="${PIPELINE_SCHEDULE}" \
          --worker-region="${REGION}" \
          --disable-public-ips \
          --additional-experiments=${PIPELINE_EXPERIMENTS} \
          --parameters="${PIPELINE_PARAMETERS}"
